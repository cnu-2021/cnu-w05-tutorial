###week05_ex1_start_py
# Create an x-axis with 1000 points
x = np.linspace(-np.pi, np.pi, 1000)

# Evaluate the functions at all these points
f1 = np.sin(x)
f2 = np.tan(0.49 * x)
f3 = np.sin(x) * np.cos(2*x)

# Create the plots in the same axes
plt.plot(x, f1, 'r-.')
plt.plot(x, f2, 'g:')
plt.plot(x, f3, 'b--')

# Display the plot
plt.show()
###week05_ex1_end

###week05_ex2_start_py
import matplotlib.pyplot as plt
import numpy as np

# Create an x-axis with 1000 points
x = np.linspace(-np.pi, np.pi, 1000)

# Evaluate the functions at all these points
f1 = np.sin(x)
f2 = np.tan(0.49 * x)
f3 = np.sin(x) * np.cos(2*x)

# Create a figure with 3 subplots
fig, ax = plt.subplots(1, 3, figsize=(10, 4))

# Plot each function in a different subplot
ax[0].plot(x, f1, 'r-.')
ax[1].plot(x, f2, 'g:')
ax[2].plot(x, f3, 'b--')

# Display the plot
plt.show()
###week05_ex2_end

###week05_ex3_start_py
import matplotlib.pyplot as plt
import numpy as np

# Create an x-axis with 1000 points
x = np.linspace(-np.pi, np.pi, 1000)

# Evaluate the functions at all these points
f1 = np.sin(x)
f2 = np.tan(0.49 * x)
f3 = np.sin(x) * np.cos(2*x)

# Create a figure with 3 subplots
fig, ax = plt.subplots(1, 3, figsize=(10, 4))

# Plot each function in a different subplot
ax[0].plot(x, f1, 'r-.')
ax[1].plot(x, f2, 'g:')
ax[2].plot(x, f3, 'b--')

# Store y-axis label for each plot
y_labels = [r'$f_1(x)$', r'$f_2(x)$', r'$f_3(x)$']

# Set all 3 properties for the 3 plots
for i in range(3):
    ax[i].set_xlim([-np.pi, np.pi])
    ax[i].set_xlabel(r'$x$', fontsize=14)
    ax[i].set_ylabel(y_labels[i], fontsize=14)

# Make some space
plt.subplots_adjust(hspace=0.5, wspace=0.5)

# Display the plot
plt.show()
###week05_ex3_end

###week05_ex4_start_py
import matplotlib.pyplot as plt
import numpy as np
import math

# Define a function for the truncated Maclaurin series
def trunc_cos(x, n):
    '''
    Return the truncated Maclaurin series for
    cos(x), with terms up until order n.
    '''
    cos_series = 0
    for k in range(n//2 + 1):
        # Add each term of the series up to nth order
        cos_series += (-1)**k * x**(2*k) / math.factorial(2*k)
    
    return cos_series


# Create an x-axis with 1000 points
x = np.linspace(-np.pi, np.pi, 1000)

# Create a figure
fig, ax = plt.subplots()

# Plot the requested functions
ax.plot(x, np.cos(x), 'k-', label=r'$\cos(x)$')
ax.plot(x, trunc_cos(x, 2), 'r--', label=r'Order 2')
ax.plot(x, trunc_cos(x, 4), 'g-.', label=r'Order 4')
ax.plot(x, trunc_cos(x, 6), 'b:', label=r'Order 6')

# Set axis properties
ax.set_xlim([-np.pi, np.pi])
ax.set_ylim([-1.5, 1.5])
ax.set_xlabel(r'$x$', fontsize=12)
ax.legend()

plt.show()
###week05_ex4_end

###week05_ex5_start_py
import matplotlib.pyplot as plt
import numpy as np

# Let's write a convenience function
def f(x):
    # Set coefficients
    a, b, c = -1, 3, 5
    
    # Compute the roots
    sqrt_delta = np.sqrt(b**2 - 4*a*c)
    roots = [(-b - sqrt_delta)/(2 * a), (-b + sqrt_delta)/(2 * a)]
    
    # Return the polynomial and the 2 roots
    return a*x**2 + b*x + c, roots

# Create an x-axis, compute f(x) and both roots
x = np.linspace(-4, 5, 100)
y, roots = f(x)

# Create the figure and axes
fig, ax = plt.subplots(1, 1, figsize=(9, 5))

# Plot the function and the roots
ax.plot(x, y, '--', color='deepskyblue', label=r'$f(x) = -x^2 + 3x + 5$')
ax.plot(x, np.zeros(x.shape[0]), 'k-', label=r'$y = 0$')
ax.plot(roots[0], 0, 'magenta', label='First root', marker='^', markersize=10)
ax.plot(roots[1], 0, 'magenta', label='Second root', marker='^', markersize=10)

# Tidy up the plot
ax.set_xlim([-4, 5])
ax.set_ylim([y[0], 10])
ax.set_xticks(range(-4, 6))
ax.set_xlabel(r'$x$', fontsize=14)
ax.set_ylabel(r'$f(x)$', fontsize=14)
ax.set_title('Polynomial roots', fontsize=14)
ax.legend(loc='lower center')
ax.grid(True)

plt.show()
###week05_ex5_end

###week05_ex6_start_md
Increasing the number of partitions $N$, i.e. decreasing the width of the partitions, is a straightforward way to obtain better accuracy using these methods. For instance, with $N = 300$, we get the correct value within $10^{-2}$.

We can calculate and plot the error on a log-log scale for different values of N:

###week05_ex6_switch_py
def riemann_sum(f, a, b, N, direction):
    '''
    Returns an estimation of the integral of f over [a, b]
    using left or right Riemann sum with N intervals.
    '''
    # Calculate the nodes and weights
    x_node = np.linspace(a, b, N+1)
    h = (b - a) / N
    
    # Compute the sum depending on the method
    if direction == 'left':
        x_node = x_node[:-1]
    elif direction == 'right':
        x_node = x_node[1:]
    else:
        print('Choose \'left\' or \'right\' for direction.')
        return
    
    return np.sum(h * f(x_node))


# Test accuracy with different values of N: 4, 8, 16, 32...
err = []
N_vals = []
for i in range(2, 11):
    N = 2**i
    N_vals.append(N)
    err.append(abs(I_exact - riemann_sum(f, 0, 3, N, 'left')))

# Plot log(N) vs. log(err)
fig, ax = plt.subplots()
ax.plot(np.log(N_vals), np.log(err), 'gx', label='log(error)')

# Fit a line (a deg. 1 polynomial) through the points
line_coeffs = np.polyfit(np.log(N_vals), np.log(err), 1)

# Plot the line on the same graph
x_plot = np.linspace(1, 8, 100)
line_plot = np.polyval(line_coeffs, x_plot)
ax.plot(x_plot, line_plot, 'r-', label='Line of best fit')

ax.legend()

print(f'The slope is {line_coeffs[0]:.6f}.')
plt.show()
###week05_ex6_switch_md
A line of slope $-1$ means that the error decreases **linearly** with increasing N.
###week05_ex6_end

###week05_ex7_start_py
We can calculate and plot the error on a log-log scale for different values of N:

###week05_ex7_switch_py
def midpoint(f, a, b, N):
    '''
    Returns an estimation of the integral of f over [a, b]
    using the midpoint rule with N intervals.
    '''
    # Calculate the nodes
    h = (b - a) / N
    x_node = np.linspace(a + 0.5*h, b, N)
    
    # Compute the sum and return it
    return np.sum(h * f(x_node))


# Test accuracy with different values of N: 4, 8, 16, 32...
err = []
N_vals = []
for i in range(2, 11):
    N = 2**i
    N_vals.append(N)
    err.append(abs(I_exact - midpoint(f, 0, 3, N)))

# Plot log(N) vs. log(err)
fig, ax = plt.subplots()
ax.plot(np.log(N_vals), np.log(err), 'gx', label='log(error)')

# Fit a line (a deg. 1 polynomial) through the points
line_coeffs = np.polyfit(np.log(N_vals), np.log(err), 1)

# Plot the line on the same graph
x_plot = np.linspace(1, 8, 100)
line_plot = np.polyval(line_coeffs, x_plot)
ax.plot(x_plot, line_plot, 'r-', label='Line of best fit')

ax.legend()

print(f'The slope is {line_coeffs[0]:.6f}.')
plt.show()
###week05_ex7_switch_md
A line of slope $-1$ means that the error decreases **linearly** with increasing N.
###week05_ex7_end

###week05_ex8_start_py
def trapezoid(f, a, b, N):
    '''
    Returns an estimation of the integral of f over [a, b]
    using the trapezoid rule with N intervals.
    '''
    # Calculate the nodes
    x_node = np.linspace(a, b, N+1)
    h = (b - a) / N
    
    # Compute the sum and return it
    return np.sum(0.5 * h * (f(x_node[:-1]) + f(x_node[1:])))


# Test accuracy with different values of N: 4, 8, 16, 32...
err = []
N_vals = []
for i in range(2, 16):
    N = 2**i
    N_vals.append(N)
    err.append(abs(I_exact - trapezoid(f, 0, 3, N)))

# Plot log(N) vs. log(err)
fig, ax = plt.subplots()
ax.plot(np.log(N_vals), np.log(err), 'gx', label='log(error)')

# Fit a line (a deg. 1 polynomial) through the points
line_coeffs = np.polyfit(np.log(N_vals), np.log(err), 1)

# Plot the line on the same graph
x_plot = np.linspace(1, 12, 100)
line_plot = np.polyval(line_coeffs, x_plot)
ax.plot(x_plot, line_plot, 'r-', label='Line of best fit')

ax.legend()

print(f'The slope is {line_coeffs[0]:.6f}.')
plt.show()
###week05_ex8_switch_md
A slope of -2 signifies that the error decreases with $N^2$. This means that the trapezoid rule gives a more accurate estimate than Riemann or midpoint rules, for smaller values of $N$.
###week05_ex8_end
